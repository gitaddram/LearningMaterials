{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f85f8ee3-13c6-47c8-8e5a-5ca646cd48fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "coagulation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ram/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# create an object of class WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"playing\", 'v'))\n",
    "print(lemmatizer.lemmatize(\"coagulation\",'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60cfa2a1-8d61-41ef-a937-016359c523d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased text: donal trump has issued a flurry of executive orders on everything from immigration, climate to pardons after being sworn in as the 47th president of united states.\n",
      "Text without punctuation: donal trump has issued a flurry of executive orders on everything from immigration climate to pardons after being sworn in as the 47th president of united states\n",
      "Tokenized words: ['donal', 'trump', 'has', 'issued', 'a', 'flurry', 'of', 'executive', 'orders', 'on', 'everything', 'from', 'immigration', 'climate', 'to', 'pardons', 'after', 'being', 'sworn', 'in', 'as', 'the', '47th', 'president', 'of', 'united', 'states']\n",
      "Text without stopwords: ['donal', 'trump', 'issued', 'flurry', 'executive', 'orders', 'everything', 'immigration', 'climate', 'pardons', 'sworn', '47th', 'president', 'united', 'states']\n",
      "Stemmed words: ['donal', 'trump', 'issu', 'flurri', 'execut', 'order', 'everyth', 'immigr', 'climat', 'pardon', 'sworn', '47th', 'presid', 'unit', 'state']\n",
      "Lemmatized words: ['donal', 'trump', 'issued', 'flurry', 'executive', 'order', 'everything', 'immigration', 'climate', 'pardon', 'sworn', '47th', 'president', 'united', 'state']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ram/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ram/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ram/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Normalization\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Sample text\n",
    "text=\"Donal Trump has issued a flurry of executive orders on everything from immigration, climate to pardons after being sworn in as the 47th President of United States.\"\n",
    "# Lowercasing\n",
    "text_lower = text.lower()\n",
    "print(\"Lowercased text:\", text_lower)\n",
    "\n",
    "# Removing punctuation\n",
    "text_no_punct = re.sub(r'[^\\w\\s]', '', text_lower)\n",
    "print(\"Text without punctuation:\", text_no_punct)\n",
    "\n",
    "# Tokenization\n",
    "words = nltk.word_tokenize(text_no_punct)\n",
    "print(\"Tokenized words:\", words)\n",
    "\n",
    "# Removing stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words_no_stop = [word for word in words if word not in stop_words]\n",
    "print(\"Text without stopwords:\", words_no_stop)\n",
    "\n",
    "# Stemming\n",
    "ps = PorterStemmer()\n",
    "words_stemmed = [ps.stem(word) for word in words_no_stop]\n",
    "print(\"Stemmed words:\", words_stemmed)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words_lemmatized = [lemmatizer.lemmatize(word) for word in words_no_stop]\n",
    "print(\"Lemmatized words:\", words_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a35704c0-d992-488d-a04c-82a73178bf99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Narendra', 'NNP'), ('Modi', 'NNP'), ('is', 'VBZ'), ('Prime', 'NNP'), ('Minister', 'NNP'), ('of', 'IN'), ('India', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# POS-Tagging\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text = \"Narendra Modi is Prime Minister of India.\"\n",
    "tokenized_text = word_tokenize(text)\n",
    "tags = tokens_tag = pos_tag(tokenized_text)\n",
    "print(str(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72e474aa-bdd0-4db2-825a-7f2bc54320d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names (Words):  ['celestial' 'curicial' 'day' 'development' 'different' 'discoverd'\n",
      " 'every' 'followed' 'gravity' 'issac' 'newton' 'objects' 'of' 'people'\n",
      " 'physics' 'redefining' 'rules' 'than' 'that' 'the' 'this' 'thought' 'to'\n",
      " 'understanding' 'was']\n",
      "\n",
      "Bag of Words Representation:\n",
      "[[0 0 0 0 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 2 1 1 1 0 1 0 0 0]\n",
      " [0 1 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# BoW Implementation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text data\n",
    "documents = [\n",
    "\t\"Issac Newton discoverd gravity.\",\n",
    "\t\"People thought the celestial objects followed different rules than that of every day rules\",\n",
    "\t\"Redefining this understanding was curicial to development of Physics\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the model and transform the documents into a Bag of Words\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "# get the feature names (i.e., unique words in the corpus)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the Bag of Words matrix into an array\n",
    "bow_array = bow_matrix.toarray()\n",
    "\n",
    "# Display the Bag of Words\n",
    "print(\"Feature Names (Words): \", feature_names)\n",
    "print(\"\\nBag of Words Representation:\")\n",
    "print(bow_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60ee6c11-67e3-4fa9-9ed0-f8c588638dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names: ['celestial' 'curicial' 'day' 'development' 'different' 'discoverd'\n",
      " 'every' 'followed' 'gravity' 'issac' 'newton' 'objects' 'of' 'people'\n",
      " 'physics' 'redefining' 'rules' 'than' 'that' 'the' 'this' 'thought' 'to'\n",
      " 'understanding' 'was']\n",
      "IF-IDF array:\n",
      "[[0.         0.         0.         0.         0.         0.5\n",
      "  0.         0.         0.5        0.5        0.5        0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.25336031 0.         0.25336031 0.         0.25336031 0.\n",
      "  0.25336031 0.25336031 0.         0.         0.         0.25336031\n",
      "  0.19268705 0.25336031 0.         0.         0.50672062 0.25336031\n",
      "  0.25336031 0.25336031 0.         0.25336031 0.         0.\n",
      "  0.        ]\n",
      " [0.         0.34142622 0.         0.34142622 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.25966344 0.         0.34142622 0.34142622 0.         0.\n",
      "  0.         0.         0.34142622 0.         0.34142622 0.34142622\n",
      "  0.34142622]]\n"
     ]
    }
   ],
   "source": [
    "# Tfidf implementation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample text data\n",
    "documents = [\n",
    "\t\"Issac Newton discoverd gravity.\",\n",
    "\t\"People thought the celestial objects followed different rules than that of every day rules\",\n",
    "\t\"Redefining this understanding was curicial to development of Physics\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "print(\"Feature names:\",feature_names)\n",
    "print(\"IF-IDF array:\")\n",
    "print(tfidf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9eb18f35-cce5-47f3-97da-86986eb3f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: \n",
      "[('Twilight',), ('is',), ('a',), ('mater',), ('of',), ('disguise',), ('who',), ('has',), ('ten',), ('thousand',), ('faces',), ('.',)]\n",
      "\n",
      "Unigram Frequencies: \n",
      "Counter({('Twilight',): 1, ('is',): 1, ('a',): 1, ('mater',): 1, ('of',): 1, ('disguise',): 1, ('who',): 1, ('has',): 1, ('ten',): 1, ('thousand',): 1, ('faces',): 1, ('.',): 1})\n",
      "Bigrams: \n",
      "[('Twilight', 'is'), ('is', 'a'), ('a', 'mater'), ('mater', 'of'), ('of', 'disguise'), ('disguise', 'who'), ('who', 'has'), ('has', 'ten'), ('ten', 'thousand'), ('thousand', 'faces'), ('faces', '.')]\n",
      "\n",
      "Bigram Frequencies: \n",
      "Counter({('Twilight', 'is'): 1, ('is', 'a'): 1, ('a', 'mater'): 1, ('mater', 'of'): 1, ('of', 'disguise'): 1, ('disguise', 'who'): 1, ('who', 'has'): 1, ('has', 'ten'): 1, ('ten', 'thousand'): 1, ('thousand', 'faces'): 1, ('faces', '.'): 1})\n",
      "Trigrams: \n",
      "[('Twilight', 'is', 'a'), ('is', 'a', 'mater'), ('a', 'mater', 'of'), ('mater', 'of', 'disguise'), ('of', 'disguise', 'who'), ('disguise', 'who', 'has'), ('who', 'has', 'ten'), ('has', 'ten', 'thousand'), ('ten', 'thousand', 'faces'), ('thousand', 'faces', '.')]\n",
      "\n",
      "Trigram Frequencies: \n",
      "Counter({('Twilight', 'is', 'a'): 1, ('is', 'a', 'mater'): 1, ('a', 'mater', 'of'): 1, ('mater', 'of', 'disguise'): 1, ('of', 'disguise', 'who'): 1, ('disguise', 'who', 'has'): 1, ('who', 'has', 'ten'): 1, ('has', 'ten', 'thousand'): 1, ('ten', 'thousand', 'faces'): 1, ('thousand', 'faces', '.'): 1})\n"
     ]
    }
   ],
   "source": [
    "# N-grams\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "# Sample text data\n",
    "text = \"Twilight is a mater of disguise who has ten thousand faces.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Generate Unigrams (1-gram)\n",
    "unigrams = list(ngrams(tokens,1))\n",
    "unigram_freq = Counter(unigrams)\n",
    "print(\"Unigrams: \")\n",
    "print(unigrams)\n",
    "print(\"\\nUnigram Frequencies: \")\n",
    "print(unigram_freq)\n",
    "\n",
    "# Generate Unigrams (1-gram)\n",
    "bigrams = list(ngrams(tokens,2))\n",
    "bigram_freq = Counter(bigrams)\n",
    "print(\"Bigrams: \")\n",
    "print(bigrams)\n",
    "print(\"\\nBigram Frequencies: \")\n",
    "print(bigram_freq)\n",
    "\n",
    "# Generate Unigrams (1-gram)\n",
    "trigrams = list(ngrams(tokens,3))\n",
    "trigram_freq = Counter(trigrams)\n",
    "print(\"Trigrams: \")\n",
    "print(trigrams)\n",
    "print(\"\\nTrigram Frequencies: \")\n",
    "print(trigram_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b404056-7d19-4c66-802b-f2b00468c5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Embeddings for 'physics': \n",
      "[-8.6189089e-03  3.6706424e-03  5.1883548e-03  5.7437806e-03\n",
      "  7.4650729e-03 -6.1647915e-03  1.1066109e-03  6.0496745e-03\n",
      " -2.8434854e-03 -6.1756647e-03 -4.1246202e-04 -8.3710104e-03\n",
      " -5.5971048e-03  7.1038795e-03  3.3552283e-03  7.2207232e-03\n",
      "  6.8033212e-03  7.5318124e-03 -3.7981474e-03 -5.6852534e-04\n",
      "  2.3434735e-03 -4.5170793e-03  8.3869314e-03 -9.8585887e-03\n",
      "  6.7623998e-03  2.9124599e-03 -4.9319174e-03  4.3944260e-03\n",
      " -1.7434919e-03  6.7167045e-03  9.9692075e-03 -4.3728347e-03\n",
      " -6.0278189e-04 -5.7023047e-03  3.8443655e-03  2.7906538e-03\n",
      "  6.8925591e-03  6.1067273e-03  9.5394002e-03  9.2687886e-03\n",
      "  7.8987256e-03 -6.9909682e-03 -9.1602998e-03 -3.4999350e-04\n",
      " -3.0998802e-03  7.8903008e-03  5.9339083e-03 -1.5478234e-03\n",
      "  1.5130843e-03  1.7931361e-03  7.8171734e-03 -9.5184054e-03\n",
      " -2.0762160e-04  3.4672646e-03 -9.4560062e-04  8.3853193e-03\n",
      "  9.0160174e-03  6.5322290e-03 -7.1871141e-04  7.7075576e-03\n",
      " -8.5300328e-03  3.2060805e-03 -4.6301228e-03 -5.0918125e-03\n",
      "  3.5864834e-03  5.3803031e-03  7.7705239e-03 -5.7641575e-03\n",
      "  7.4286596e-03  6.6218860e-03 -3.7063537e-03 -8.7402230e-03\n",
      "  5.4397900e-03  6.5093618e-03 -7.8668282e-04 -6.7022820e-03\n",
      " -7.0834756e-03 -2.4916145e-03  5.1411362e-03 -3.6674151e-03\n",
      " -9.3709109e-03  3.8221828e-03  4.8833871e-03 -6.4309300e-03\n",
      "  1.2088807e-03 -2.0774931e-03  2.2187314e-05 -9.8838340e-03\n",
      "  2.6941113e-03 -4.7479384e-03  1.0891294e-03 -1.5700581e-03\n",
      "  2.1965543e-03 -7.8803282e-03 -2.7111550e-03  2.6643239e-03\n",
      "  5.3457730e-03 -2.3932399e-03 -9.5104873e-03  4.5044981e-03]\n",
      "\n",
      "Words most similar to 'physics': \n",
      "[('curcial', 0.18905824422836304), ('every', 0.18849390745162964), ('thought', 0.16071072220802307), ('followed', 0.15948139131069183), ('people', 0.13729728758335114)]\n"
     ]
    }
   ],
   "source": [
    "# Word Embeddings from Word2Vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Sample text data\n",
    "text = [\n",
    "\t\"Issac Newton discoverd gravity and changed our understanding of physics.\",\n",
    "\t\"People thought the celestial objects followed different rules than that of every day rules of physics\",\n",
    "\t\"Redefining this understanding was curcial to development of Physics\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "tokenized_text = [word_tokenize(sentence.lower()) for sentence in text]\n",
    "\n",
    "# Train a Word2Vec model on the tokenized text\n",
    "model = Word2Vec(tokenized_text, vector_size=100, window=3, min_count=1, sg=0)\n",
    "\n",
    "# Get word embeddings for a specific word\n",
    "physics_vector = model.wv['physics']\n",
    "print(\"Word Embeddings for 'physics': \")\n",
    "print(physics_vector)\n",
    "\n",
    "# Words most similar to physics\n",
    "similar_words = model.wv.most_similar('physics',topn=5)\n",
    "print(\"\\nWords most similar to 'physics': \")\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7956961-3cea-4d39-9fb3-e1b9f7eae414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of last hidden states:  torch.Size([1, 15, 768])\n",
      "Token: [CLS]\n",
      "Embedding: [-0.75372785  0.21267381 -0.46011588  0.3534106 ]...\n",
      "\n",
      "Token: iss\n",
      "Embedding: [-1.0019723  -0.09209419  0.03805587 -1.1330551 ]...\n",
      "\n",
      "Token: ##ac\n",
      "Embedding: [ 0.04256701  0.7425329  -0.74694717 -0.48725405]...\n",
      "\n",
      "Token: newton\n",
      "Embedding: [ 0.3416494 -0.0373348 -0.9118464 -0.6818827]...\n",
      "\n",
      "Token: discover\n",
      "Embedding: [-1.1914814   0.9903251  -0.823267   -0.09535353]...\n",
      "\n",
      "Token: ##d\n",
      "Embedding: [-0.8980761   0.63520557 -0.4502475  -0.03369765]...\n",
      "\n",
      "Token: gravity\n",
      "Embedding: [ 0.0937224   0.56387097 -0.03989023 -0.763481  ]...\n",
      "\n",
      "Token: and\n",
      "Embedding: [-6.7916501e-01 -1.2992848e-01 -1.0779214e+00  8.6374022e-04]...\n",
      "\n",
      "Token: changed\n",
      "Embedding: [-0.05747977  0.09054674 -0.3289726   0.13091314]...\n",
      "\n",
      "Token: our\n",
      "Embedding: [ 0.3833255   0.37291035 -0.17615962  0.41101083]...\n",
      "\n",
      "Token: understanding\n",
      "Embedding: [ 0.17283192  1.0845681  -0.3424585  -0.18267825]...\n",
      "\n",
      "Token: of\n",
      "Embedding: [-0.19235352  0.9480484   0.10838635 -0.23513459]...\n",
      "\n",
      "Token: physics\n",
      "Embedding: [ 0.3323938   0.49819502 -0.602781   -0.56412864]...\n",
      "\n",
      "Token: .\n",
      "Embedding: [ 0.24010733 -0.09119019 -0.29503432  0.15611741]...\n",
      "\n",
      "Token: [SEP]\n",
      "Embedding: [ 0.6819432  -0.0602207  -0.19833511  0.46805054]...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Contextual word embeddings using BERT\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "# Example sentence\n",
    "sentence = \t\"Issac Newton discoverd gravity and changed our understanding of physics.\"\n",
    "\n",
    "# Tokenize the input sentence and conver tokens to tensor\n",
    "input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
    "\n",
    "# Pass the input through the BERT model to get embeddings\n",
    "with torch.no_grad():\n",
    "\toutputs = model(input_ids)\n",
    "\tlast_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Print the shape of the last hidden states tensor\n",
    "print(\"shape of last hidden states: \", last_hidden_states.shape)\n",
    "\n",
    "# Convert the embeddings into numpy array\n",
    "embeddings = last_hidden_states.squeeze().numpy()\n",
    "\n",
    "# Tokenize the sentence to match the embeddins to the word\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze())\n",
    "\n",
    "# Print the tokens and their corresponding contextual embeddings\n",
    "for token, embedding in zip(tokens, embeddings):\n",
    "\tprint(f\"Token: {token}\")\n",
    "\tprint(f\"Embedding: {embedding[:4]}...\")\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40c25669-788c-4ad6-8d65-92a9d76e4b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities in the sentence: \n",
      "Narendra Modi:PERSON\n",
      "3:CARDINAL\n",
      "India:GPE\n"
     ]
    }
   ],
   "source": [
    "# NER\n",
    "import spacy\n",
    "\n",
    "# Load the pre-trained NLP model from spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# The sentence for which we want to perform NER\n",
    "sentence = \"Narendra Modi serverd for 3 consecutive terms as Prime Minister of India.\"\n",
    "\n",
    "# Process the sentence using the NLP model\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Print the names entities recognized in the sentence\n",
    "print(\"Named Entities in the sentence: \")\n",
    "for ent in doc.ents:\n",
    "\tprint(f\"{ent.text}:{ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e443f32-1a0f-4dae-8741-dede2ecd2424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sentence:  megumi tried to get control me\n"
     ]
    }
   ],
   "source": [
    "# Sentence Generation using Bigram Model\n",
    "import nltk\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "# Sample text data (corpus)\n",
    "corpus = [\n",
    "\t\"I ate Sukuna finger\",\n",
    "\t\"Sukuna can control me\",\n",
    "\t\"Megumi tried to fight Sukuna\",\n",
    "\t\"Sukuna is really strong\",\n",
    "\t\"I fight with Sukuna to get control\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences into words\n",
    "tokenized_corpus = [nltk.word_tokenize(sentence.lower()) for sentence in corpus]\n",
    "\n",
    "# Create bigrams from the tokenized corpus\n",
    "bigrams = []\n",
    "for sentence in tokenized_corpus:\n",
    "    bigrams.extend(list(nltk.bigrams(sentence)))\n",
    "\n",
    "# Calculate bigram frequencies\n",
    "bigram_freq = defaultdict(Counter)\n",
    "for w1,w2 in bigrams:\n",
    "\tbigram_freq[w1][w2] += 1\n",
    "\n",
    "# Calculate bigram probabilities\n",
    "bigram_prob = defaultdict(dict)\n",
    "for w1 in bigram_freq:\n",
    "\ttotal_count = float(sum(bigram_freq[w1].values()))\n",
    "\tfor w2 in bigram_freq[w1]:\n",
    "\t\tbigram_prob[w1][w2] = bigram_freq[w1][w2]/total_count\n",
    "\n",
    "# Function to generate text using the bigram model\n",
    "def generate_sentence(start_word, num_words=10):\n",
    "    current_word = start_word\n",
    "    sentence = [current_word]\n",
    "    for _ in range (num_words - 1):\n",
    "        if current_word in bigram_prob:\n",
    "            next_word = random.choices(list(bigram_prob[current_word].keys()),list(bigram_prob[current_word].values()))[0]\n",
    "            sentence.append(next_word)\n",
    "            current_word = next_word\n",
    "        else:\n",
    "            break\n",
    "    return ' '.join(sentence) \n",
    "\n",
    "# Generate a sentence starting with \"i\"\n",
    "generated_sentence = generate_sentence(\"megumi\", num_words=10)\n",
    "print(\"Generated sentence: \", generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1519add0-8f07-4b38-998b-bb7f68f6b1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "# implementing a simple transformer model*\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, ffn_dim, max_seq_len, num_classes):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, model_dim)\n",
    "        self.model_dim = model_dim\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerLayer(model_dim, num_heads, ffn_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(model_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        positional_encoding = self._generate_positional_encoding(seq_len)\n",
    "        x = self.embedding(x) + positional_encoding\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.fc_out(x.mean(dim=1))\n",
    "    \n",
    "    def _generate_positional_encoding(self, seq_len):\n",
    "        positional_encoding = torch.zeros(seq_len, self.model_dim)\n",
    "        for pos in range(seq_len):\n",
    "            for i in range(0, self.model_dim, 2):\n",
    "                positional_encoding[pos, i] = math.sin(pos / (10000 ** (i / self.model_dim)))\n",
    "                positional_encoding[pos, i + 1] = math.cos(pos / (10000 ** ((i + 1) / self.model_dim)))\n",
    "        return positional_encoding.unsqueeze(0)\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads, ffn_dim):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=model_dim, num_heads=num_heads)\n",
    "        self.norm1 = nn.LayerNorm(model_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(model_dim, ffn_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ffn_dim, model_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(model_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.multihead_attention(x, x, x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "        return x\n",
    "\n",
    "# Example usage:\n",
    "input_dim = 10000  # Vocabulary size\n",
    "model_dim = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "ffn_dim = 2048\n",
    "max_seq_len = 100\n",
    "num_classes = 10\n",
    "\n",
    "# Instantiate the model\n",
    "model = TransformerModel(input_dim, model_dim, num_heads, num_layers, ffn_dim, max_seq_len, num_classes)\n",
    "\n",
    "# Example input (batch_size=32, sequence_length=50)\n",
    "x = torch.randint(0, input_dim, (32, 50))\n",
    "\n",
    "# Forward pass\n",
    "output = model(x)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab36cc5-f220-4766-ad64-2a5d3bd799ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(chatbot_env)",
   "language": "python",
   "name": "chatbot_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
